<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EngageSense: Real-Time Voice Sentiment & Visualization</title>
    <script src="https://unpkg.com/wavesurfer.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://unpkg.com/meyda/dist/meyda.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/css/bootstrap.min.css">
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #121212;
            color: #fff;
            margin: 0;
            padding: 0;
        }
        header {
            text-align: center;
            padding: 20px;
            background-color: #1f1f1f;
            color: #0f0;
        }
        .container {
            max-width: 900px;
            margin: 20px auto;
            background-color: #1f1f1f;
            padding: 20px;
            border-radius: 10px;
        }
        canvas {
            background: #000;
            border: 2px solid #0f0;
            border-radius: 5px;
        }
        .progress-bar {
            height: 5px;
            margin-top: 10px;
        }
    </style>
</head>
<body>
    <header>
        <h1>EngageSense: Real-Time Voice Sentiment & Visualization</h1>
    </header>
    <div class="container text-center">
        <h3>Record or Upload Audio</h3>
        <div class="btn-group">
            <button id="recordBtn" class="btn btn-success">Start Recording</button>
            <button id="stopBtn" class="btn btn-danger" disabled>Stop</button>
            <button id="uploadBtn" class="btn btn-light">Upload Audio</button>
            <input type="file" id="fileInput" accept="audio/*" class="d-none">
        </div>
        <canvas id="audioVisualization" width="800" height="200"></canvas>
        <div class="results mt-4">
            <h4>Analysis Results</h4>
            <p>Loudness (RMS): <strong id="rms">N/A</strong></p>
            <p>Spectral Centroid: <strong id="centroid">N/A</strong></p>
            <p>Pitch: <strong id="pitch">N/A</strong></p>
            <p>Sentiment: <strong id="sentiment">N/A</strong></p>
        </div>
        <canvas id="sentimentChart" width="400" height="200"></canvas>
    </div>

    <script>
        // DOM Elements
        const recordBtn = document.getElementById('recordBtn');
        const stopBtn = document.getElementById('stopBtn');
        const uploadBtn = document.getElementById('uploadBtn');
        const fileInput = document.getElementById('fileInput');
        const canvas = document.getElementById('audioVisualization');
        const ctx = canvas.getContext('2d');

        let audioContext, analyzer, meydaAnalyzer, recorder, chunks = [];
        let sentimentModel;

        async function loadSentimentModel() {
            sentimentModel = await tf.loadLayersModel('https://storage.googleapis.com/tfjs-models/tfjs/sentiment_cnn_v1/model.json');
        }

        // Real-time Visualization Function
        function visualizeAudio() {
            const bufferLength = analyzer.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);

            function draw() {
                analyzer.getByteFrequencyData(dataArray);
                ctx.fillStyle = 'black';
                ctx.fillRect(0, 0, canvas.width, canvas.height);

                const barWidth = (canvas.width / bufferLength) * 2.5;
                let x = 0;

                for (let i = 0; i < bufferLength; i++) {
                    const barHeight = dataArray[i];
                    ctx.fillStyle = `rgb(${barHeight + 100}, 50, 255)`;
                    ctx.fillRect(x, canvas.height - barHeight / 2, barWidth, barHeight / 2);
                    x += barWidth + 1;
                }
                requestAnimationFrame(draw);
            }
            draw();
        }

        // Start Recording
        recordBtn.addEventListener('click', async () => {
            chunks = [];
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            audioContext = new AudioContext();
            analyzer = audioContext.createAnalyser();
            const source = audioContext.createMediaStreamSource(stream);
            source.connect(analyzer);

            meydaAnalyzer = Meyda.createMeydaAnalyzer({
                audioContext: audioContext,
                source: source,
                bufferSize: 512,
                featureExtractors: ["rms", "spectralCentroid", "pitch"],
                callback: updateMetrics
            });
            meydaAnalyzer.start();

            recorder = new MediaRecorder(stream);
            recorder.ondataavailable = e => chunks.push(e.data);
            recorder.start();

            visualizeAudio();
            recordBtn.disabled = true;
            stopBtn.disabled = false;
        });

        // Stop Recording
        stopBtn.addEventListener('click', () => {
            recorder.stop();
            meydaAnalyzer.stop();
            recordBtn.disabled = false;
            stopBtn.disabled = true;

            recorder.onstop = () => {
                const blob = new Blob(chunks, { type: 'audio/wav' });
                const url = URL.createObjectURL(blob);
                analyzeSentiment(blob);
            };
        });

        // Upload File
        uploadBtn.addEventListener('click', () => fileInput.click());
        fileInput.addEventListener('change', () => {
            const file = fileInput.files[0];
            analyzeSentiment(file);
        });

        // Update Metrics
        function updateMetrics(features) {
            document.getElementById('rms').innerText = features.rms.toFixed(4);
            document.getElementById('centroid').innerText = features.spectralCentroid.toFixed(2);
            document.getElementById('pitch').innerText = features.pitch.toFixed(2);
        }

        // Analyze Sentiment
        async function analyzeSentiment(audioFile) {
            const tensor = tf.tensor1d([0.6, 0.8, 0.7]); // Replace with feature extraction logic
            const prediction = sentimentModel.predict(tensor.expandDims(0)).arraySync()[0][0];
            const sentiment = prediction > 0.5 ? "Positive" : "Negative";

            document.getElementById('sentiment').innerText = sentiment;
            updateSentimentChart(prediction * 100);
        }

        // Update Sentiment Chart
        let chart;
        function updateSentimentChart(score) {
            const ctx = document.getElementById('sentimentChart').getContext('2d');
            if (chart) chart.destroy();
            chart = new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: ['Sentiment Score'],
                    datasets: [{
                        label: 'Sentiment',
                        data: [score],
                        backgroundColor: score > 50 ? 'green' : 'red'
                    }]
                }
            });
        }

        // Load Sentiment Model on Page Load
        loadSentimentModel();
    </script>
</body>
</html>
